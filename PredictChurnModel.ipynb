{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data and checking file data type is missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5346 entries, 0 to 5345\n",
      "Data columns (total 20 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   id                           5346 non-null   int64  \n",
      " 1   Churn                        5346 non-null   int64  \n",
      " 2   Tenure                       5094 non-null   float64\n",
      " 3   PreferredLoginDevice         5346 non-null   object \n",
      " 4   CityTier                     5346 non-null   int64  \n",
      " 5   WarehouseToHome              5109 non-null   float64\n",
      " 6   PreferredPaymentMode         5346 non-null   object \n",
      " 7   Gender                       5346 non-null   object \n",
      " 8   HourSpendOnApp               5107 non-null   float64\n",
      " 9   NumberOfDeviceRegistered     5346 non-null   int64  \n",
      " 10  PreferedOrderCat             5346 non-null   object \n",
      " 11  SatisfactionScore            5346 non-null   int64  \n",
      " 12  MaritalStatus                5346 non-null   object \n",
      " 13  NumberOfAddress              5346 non-null   int64  \n",
      " 14  Complain                     5346 non-null   int64  \n",
      " 15  OrderAmountHikeFromlastYear  5091 non-null   float64\n",
      " 16  CouponUsed                   5098 non-null   float64\n",
      " 17  OrderCount                   5095 non-null   float64\n",
      " 18  DaySinceLastOrder            5054 non-null   float64\n",
      " 19  CashbackAmount               5346 non-null   float64\n",
      "dtypes: float64(8), int64(7), object(5)\n",
      "memory usage: 835.4+ KB\n"
     ]
    }
   ],
   "source": [
    "train_filepath = \"C:/Users/user/VSCode/DATA/OLC/train.csv\"\n",
    "train_data = pd.read_csv(train_filepath)\n",
    "train_data.info()\n",
    "data_map = train_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the data descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encoding the categorical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map[\"PreferredLoginDevice\"] = data_map[\"PreferredLoginDevice\"].map({\n",
    "    \"Phone\" : 0,\n",
    "    \"Mobile Phone\" : 0,\n",
    "    \"Computer\" : 1\n",
    "})\n",
    "data_map[\"PreferredPaymentMode\"] = data_map[\"PreferredPaymentMode\"].map({\n",
    "    \"Cash on Delivery\" : 0,\n",
    "    \"COD\" : 0,\n",
    "    \"CC\" : 1,\n",
    "    \"Credit Card\" : 1,\n",
    "    \"Debit Card\" : 2,\n",
    "    \"E wallet\" : 3,\n",
    "    \"UPI\" : 4\n",
    "})\n",
    "data_map[\"Gender\"] = data_map[\"Gender\"].map({\n",
    "    \"Male\" : 0,\n",
    "    \"Female\" : 1\n",
    "}) \n",
    "data_map[\"PreferedOrderCat\"] = data_map[\"PreferedOrderCat\"].map({\n",
    "    \"Fashion\" : 0,\n",
    "    \"Grocery\" : 1,\n",
    "    \"Laptop & Accessory\" : 2,\n",
    "    \"Mobile\" : 3,\n",
    "    \"Mobile Phone\" : 3,\n",
    "    \"Others\" : 4\n",
    "})\n",
    "data_map[\"MaritalStatus\"] = data_map[\"MaritalStatus\"].map({\n",
    "    \"Single\" : 0,\n",
    "    \"Married\" : 1,\n",
    "    \"Divorced\" : 2\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Matrix to find correlation of each variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = data_map.corr()\n",
    "plt.figure(figsize=(17,15))\n",
    "sns.heatmap(corr_matrix, annot=True, square=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix of Every Variables', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplot to find the outliers and the data spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = data_map.select_dtypes(include=['int64', 'float64']).columns\n",
    "num_columns = num_columns.drop(['id'])\n",
    "# Set up subplots with more space between subplots\n",
    "num_plots = len(num_columns)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=num_plots, figsize=(45, 10))\n",
    "\n",
    "# Create box plots for each numerical column in a subplot\n",
    "for i, column in enumerate(num_columns):\n",
    "    sns.boxplot(y=column, data=data_map, ax=axes[i])\n",
    "    axes[i].set_title(f'Box plot for {column}')\n",
    "\n",
    "plt.tight_layout(pad=5.0)  # Adjust the pad parameter for spacing between subplots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairplot of every single combination of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Density Plot to chek the variable's density compared to churn variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots = len(num_columns)\n",
    "num_cols = 2  # Number of columns in each row\n",
    "num_rows = num_plots // num_cols + (num_plots % num_cols > 0)  # Calculate the number of rows needed\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(35, 15))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, column in enumerate(num_columns):\n",
    "    sns.kdeplot(data=data_map, x=column, hue=\"Churn\", fill=True, common_norm=False, ax=axes[i])\n",
    "    axes[i].set_title(f'Density plot of {column} grouped by Churn')\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using MICE (Multiple Imputation by Chained Equations) to impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map = data_map.drop('id', axis=1)\n",
    "# Create the imputer\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "# Fit the imputer and transform the data\n",
    "data_imputed = pd.DataFrame(imputer.fit_transform(data_map), columns=data_map.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filepath = \"C:/Users/user/VSCode/DATA/OLC/test.csv\"\n",
    "test = pd.read_csv(test_filepath)\n",
    "test_data = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encoding the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284 entries, 0 to 283\n",
      "Data columns (total 19 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   id                           284 non-null    int64  \n",
      " 1   Tenure                       272 non-null    float64\n",
      " 2   PreferredLoginDevice         284 non-null    int64  \n",
      " 3   CityTier                     284 non-null    int64  \n",
      " 4   WarehouseToHome              270 non-null    float64\n",
      " 5   PreferredPaymentMode         284 non-null    int64  \n",
      " 6   Gender                       284 non-null    int64  \n",
      " 7   HourSpendOnApp               268 non-null    float64\n",
      " 8   NumberOfDeviceRegistered     284 non-null    int64  \n",
      " 9   PreferedOrderCat             284 non-null    int64  \n",
      " 10  SatisfactionScore            284 non-null    int64  \n",
      " 11  MaritalStatus                284 non-null    int64  \n",
      " 12  NumberOfAddress              284 non-null    int64  \n",
      " 13  Complain                     284 non-null    int64  \n",
      " 14  OrderAmountHikeFromlastYear  274 non-null    float64\n",
      " 15  CouponUsed                   276 non-null    float64\n",
      " 16  OrderCount                   277 non-null    float64\n",
      " 17  DaySinceLastOrder            269 non-null    float64\n",
      " 18  CashbackAmount               284 non-null    float64\n",
      "dtypes: float64(8), int64(11)\n",
      "memory usage: 42.3 KB\n"
     ]
    }
   ],
   "source": [
    "test_data[\"PreferredLoginDevice\"] = test_data[\"PreferredLoginDevice\"].map({\n",
    "    \"Phone\" : 0,\n",
    "    \"Mobile Phone\" : 0,\n",
    "    \"Computer\" : 1\n",
    "})\n",
    "test_data[\"PreferredPaymentMode\"] = test_data[\"PreferredPaymentMode\"].map({\n",
    "    \"Cash on Delivery\" : 0,\n",
    "    \"COD\" : 0,\n",
    "    \"CC\" : 1,\n",
    "    \"Credit Card\" : 1,\n",
    "    \"Debit Card\" : 2,\n",
    "    \"E wallet\" : 3,\n",
    "    \"UPI\" : 4\n",
    "})\n",
    "test_data[\"Gender\"] = test_data[\"Gender\"].map({\n",
    "    \"Male\" : 0,\n",
    "    \"Female\" : 1\n",
    "}) \n",
    "test_data[\"PreferedOrderCat\"] = test_data[\"PreferedOrderCat\"].map({\n",
    "    \"Fashion\" : 0,\n",
    "    \"Grocery\" : 1,\n",
    "    \"Laptop & Accessory\" : 2,\n",
    "    \"Mobile\" : 3,\n",
    "    \"Mobile Phone\" : 3,\n",
    "    \"Others\" : 4\n",
    "})\n",
    "test_data[\"MaritalStatus\"] = test_data[\"MaritalStatus\"].map({\n",
    "    \"Single\" : 0,\n",
    "    \"Married\" : 1,\n",
    "    \"Divorced\" : 2\n",
    "})\n",
    "\n",
    "test_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputing the test data using the same method as for the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop('id', axis=1)\n",
    "# Create the imputer\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "\n",
    "# Fit the imputer and transform the data\n",
    "test_imputed = pd.DataFrame(imputer.fit_transform(test_data), columns=test_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping churn from training (X) and put it as target (y)\n",
    "\n",
    "Split the train data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_imputed.drop('Churn', axis=1)\n",
    "y = data_imputed['Churn']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Stacking to combine both model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the base models\n",
    "# base_models = [\n",
    "#     ('rf', RandomForestClassifier(\n",
    "#         n_estimators=500,\n",
    "#         max_depth=60,\n",
    "#         min_samples_split=2,\n",
    "#         min_samples_leaf=1,\n",
    "#         bootstrap=False\n",
    "#     )),\n",
    "#     ('xgb', xgb.XGBClassifier(\n",
    "#         n_estimators=700, \n",
    "#         learning_rate=0.2,\n",
    "#         max_depth=8,\n",
    "#         min_child_weight=1,\n",
    "#         gamma=0,\n",
    "#         subsample=0.8,\n",
    "#         colsample_bytree=1.0,\n",
    "#         random_state=42,\n",
    "#     ))\n",
    "# ]\n",
    "\n",
    "# # Split the data\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # # Define hyperparameter grids for each base model\n",
    "# # param_grid_rf = {\n",
    "# #     'n_estimators': [400, 500, 600, 700, 800],\n",
    "# #     'max_depth': [60, 65, 70, 80],\n",
    "# #     'min_samples_split': [2, 5, 10],\n",
    "# #     'min_samples_leaf': [1, 2, 4],\n",
    "# #     'bootstrap': [False, True]\n",
    "# # }\n",
    "\n",
    "# # param_grid_xgb = {\n",
    "# #     'n_estimators': [600, 700, 800, 900],\n",
    "# #     'learning_rate': [0.1, 0.2, 0.3, 0.5],\n",
    "# #     'max_depth': [6, 8, 10, 20, 30],\n",
    "# #     'min_child_weight': [1, 2, 4],\n",
    "# #     'gamma': [0, 0.1, 0.3, 0.5],\n",
    "# #     'subsample': [0.8, 0.9, 1.0],\n",
    "# #     'colsample_bytree': [1.0],\n",
    "# # }\n",
    "\n",
    "# # # Iterate over base models and perform hyperparameter tuning\n",
    "# # for (name, model), param_grid in zip(base_models, [param_grid_rf, param_grid_xgb]):\n",
    "# #     grid_search_base = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1', verbose=2, n_jobs=-1)\n",
    "# #     grid_search_base.fit(X_train, y_train)\n",
    "    \n",
    "# #     # Print the best parameters and score for each base model\n",
    "# #     print(f'Best Parameters for {name}: {grid_search_base.best_params_}')\n",
    "# #     print(f'Best Score for {name}: {grid_search_base.best_score_}')\n",
    "\n",
    "# #     # Update the base model with the best parameters\n",
    "# #     model.set_params(**grid_search_base.best_params_)\n",
    "    \n",
    "# #     model.fit(X_train, y_train)\n",
    "\n",
    "# # Iterate over base models and perform hyperparameter tuning\n",
    "# for (name, model) in base_models:\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "# # Create the stacking model with SVM as the final estimator\n",
    "# meta_model_svm = SVC(random_state=42)\n",
    "# stacking_model_svm = StackingClassifier(estimators=base_models, final_estimator=meta_model_svm)\n",
    "\n",
    "# # Define hyperparameter grid for stacking model with SVM\n",
    "# param_dist_svm = {\n",
    "#     'final_estimator__C': uniform(60, 120),  # Narrow down the range for C\n",
    "#     'final_estimator__kernel': ['linear', 'poly'],\n",
    "#     'final_estimator__gamma': ['scale', 'auto', 0.001, 0.01, 0.1],  # Fine-tune gamma\n",
    "#     'final_estimator__class_weight': [None, 'balanced'],\n",
    "#     'final_estimator__degree': [3, 4, 5, 6, 7, 8],  # Fine-tune the degree parameter\n",
    "#     'final_estimator__coef0': [0.1, 0.5, 1.0],  # Fine-tune the coef0 parameter\n",
    "#     'final_estimator__probability': [True, False]\n",
    "# }\n",
    "\n",
    "# # Create the RandomizedSearchCV object for stacking model with SVM\n",
    "# random_search_stacking_svm = RandomizedSearchCV(\n",
    "#     estimator=stacking_model_svm,\n",
    "#     param_distributions=param_dist_svm,\n",
    "#     n_iter= 50,  # Adjust the number of iterations\n",
    "#     cv=5,\n",
    "#     scoring='f1',\n",
    "#     verbose=2,\n",
    "#     n_jobs=-1,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Fit the RandomizedSearchCV object to the data\n",
    "# random_search_stacking_svm.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and best score for stacking model with SVM\n",
    "# print(f'Best Parameters for Stacking Model with SVM: {random_search_stacking_svm.best_params_}')\n",
    "# print(f'Best Score for Stacking Model with SVM: {random_search_stacking_svm.best_score_}')\n",
    "\n",
    "# # Make predictions and evaluate the stacking model with SVM on the validation set\n",
    "# stacking_predictions_svm = random_search_stacking_svm.best_estimator_.predict(X_val)\n",
    "\n",
    "# accuracy_stacking_svm = accuracy_score(y_val, stacking_predictions_svm)\n",
    "# precision_stacking_svm = precision_score(y_val, stacking_predictions_svm)\n",
    "# recall_stacking_svm = recall_score(y_val, stacking_predictions_svm)\n",
    "# confusion_matrix_stacking_svm = confusion_matrix(y_val, stacking_predictions_svm)\n",
    "\n",
    "# print(f\"Accuracy of Stacking Model with SVM on Validation Set: {accuracy_stacking_svm}\")\n",
    "# print(f\"Precision of Stacking Model with SVM on Validation Set: {precision_stacking_svm}\")\n",
    "# print(f\"Recall of Stacking Model with SVM on Validation Set: {recall_stacking_svm}\")\n",
    "# print(f\"Confusion Matrix of Stacking Model with SVM on Validation Set:\\n{confusion_matrix_stacking_svm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stacking Model with SVM on Validation Set: 0.9813084112149533\n",
      "Precision of Stacking Model with SVM on Validation Set: 0.9390243902439024\n",
      "Recall of Stacking Model with SVM on Validation Set: 0.9390243902439024\n",
      "Confusion Matrix of Stacking Model with SVM on Validation Set:\n",
      "[[896  10]\n",
      " [ 10 154]]\n"
     ]
    }
   ],
   "source": [
    "# Define the base models\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=70,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        bootstrap=False\n",
    "    )),\n",
    "    ('xgb', xgb.XGBClassifier(\n",
    "        n_estimators=700, \n",
    "        learning_rate=0.2,\n",
    "        max_depth=8,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=1,\n",
    "        colsample_bytree=1.0,\n",
    "        random_state=42,\n",
    "    ))\n",
    "]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Iterate over base models and perform hyperparameter tuning\n",
    "for (name, model) in base_models:\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "# Create the stacking model with SVM as the final estimator\n",
    "meta_model_svm = SVC(C= 114.05991023634516, \n",
    "                     class_weight = 'balanced', \n",
    "                     coef0 = 0.1, \n",
    "                     degree = 6, \n",
    "                     gamma = 'auto', \n",
    "                     kernel = 'poly',\n",
    "                     probability= True)\n",
    "\n",
    "stacking_model_svm = StackingClassifier(estimators=base_models, final_estimator=meta_model_svm)\n",
    "\n",
    "# Fit the stacking model with SVM to the data\n",
    "stacking_model_svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the stacking model with SVM on the validation set\n",
    "stacking_predictions_svm = stacking_model_svm.predict(X_val)\n",
    "\n",
    "accuracy_stacking_svm = accuracy_score(y_val, stacking_predictions_svm)\n",
    "precision_stacking_svm = precision_score(y_val, stacking_predictions_svm)\n",
    "recall_stacking_svm = recall_score(y_val, stacking_predictions_svm)\n",
    "confusion_matrix_stacking_svm = confusion_matrix(y_val, stacking_predictions_svm)\n",
    "\n",
    "print(f\"Accuracy of Stacking Model with SVM on Validation Set: {accuracy_stacking_svm}\")\n",
    "print(f\"Precision of Stacking Model with SVM on Validation Set: {precision_stacking_svm}\")\n",
    "print(f\"Recall of Stacking Model with SVM on Validation Set: {recall_stacking_svm}\")\n",
    "print(f\"Confusion Matrix of Stacking Model with SVM on Validation Set:\\n{confusion_matrix_stacking_svm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stacking: 0.9841121495327103\n"
     ]
    }
   ],
   "source": [
    "# Generate Predictions for Validation Set\n",
    "base_model_predictions_val = {name: model.predict(X_val) for name, model in base_models}\n",
    "base_model_predictions_array_val = np.column_stack(list(base_model_predictions_val.values()))\n",
    "\n",
    "# Fit the stacking model with SVM\n",
    "stacking_model_svm.fit(base_model_predictions_array_val, y_val)\n",
    "\n",
    "# Make Final Predictions\n",
    "stacking_predictions = stacking_model_svm.predict(base_model_predictions_array_val)\n",
    "\n",
    "# Evaluate Stacking Performance\n",
    "accuracy_stacking = accuracy_score(y_val, stacking_predictions)\n",
    "print(f\"Accuracy of Stacking: {accuracy_stacking}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the cross val of the stacking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated Accuracy Scores: [0.99065421 0.98130841 0.98598131 0.97663551 0.97663551]\n",
      "Mean Accuracy: 0.9822429906542057\n"
     ]
    }
   ],
   "source": [
    "#Apply cross-validation on the predicted test labels\n",
    "cross_val_scores = cross_val_score(stacking_model_svm, base_model_predictions_array_val, y_val, cv=5, scoring='accuracy')\n",
    "\n",
    "#Print cross-validated scores\n",
    "print(\"Cross-validated Accuracy Scores:\", cross_val_scores)\n",
    "print(\"Mean Accuracy:\", np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the final model to predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
       "       0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
       "       0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
       "       0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "       0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_test_predictions = {name: model.predict(test_imputed) for name, model in base_models}\n",
    "\n",
    "# Convert dictionary values to a NumPy array\n",
    "base_model_test_predictions_array = np.column_stack(list(base_model_test_predictions.values()))\n",
    "\n",
    "# Make final predictions on the test data using the stacking model\n",
    "stacking_test_predictions = stacking_model_svm.predict(base_model_test_predictions_array)\n",
    "display(stacking_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv(\"C:/Users/user/VSCode/PYTHON/results(hmm).csv\")\n",
    "val = val.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[165   0]\n",
      " [  4 115]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix = confusion_matrix(val, stacking_test_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the result with id to csv file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imputed['Prediction'] = stacking_test_predictions\n",
    "test_imputed['id'] = test['id']\n",
    "# Save the DataFrame to a CSV file, including only the 'ID' and 'Prediction' columns\n",
    "test_imputed[['id', 'Prediction']].to_csv('results(huh).csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Learning Curve Graph to analyze model performance changes from data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Stacking Classifier with the Meta Learner\n",
    "clf = random_search_stacking_svm.best_estimator_\n",
    "\n",
    "# Generate a learning curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(clf, X, y, cv=5)\n",
    "\n",
    "# Calculate mean and standard deviation for training set scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Calculate mean and standard deviation for test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.plot(train_sizes, train_mean, label='Training score')\n",
    "plt.plot(train_sizes, test_mean, label='Cross-validation score')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='#DDDDDD')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color='#DDDDDD')\n",
    "\n",
    "# Create plot\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Training Size')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
